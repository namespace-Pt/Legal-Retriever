{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tiger/miniconda/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "if \"..\" not in sys.path:\n",
    "    sys.path.append(\"..\")\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "import gc\n",
    "import datasets\n",
    "import json\n",
    "import string\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from safetensors.torch import load_file, save_file, safe_open\n",
    "from src import Data, Metrics, ModelArgs, get_model_and_tokenizer\n",
    "\n",
    "args = ModelArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/mnt/bn/search-douyin-rank-yg/all_data_from_lf/text_embedding_data/query_doc_info_sample_order_1028_v4\"\n",
    "\n",
    "data_files = sum([glob(f\"{base_dir}/part-{i:05d}-*.parquet\") for i in range(2)], [])\n",
    "dataset = datasets.load_dataset(\"parquet\", data_files=data_files, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 490891/490891 [01:00<00:00, 8151.52it/s]\n",
      "81it [00:05, 14.49it/s] 19000 examples [01:05, 3017.52 examples/s]\n",
      "Generating train split: 19441 examples [01:06, 290.27 examples/s] \n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import cpu_count\n",
    "from datasets import Dataset\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from glob import glob\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def collate_query_pos_neg_from_impression(impression, doc_as_query_portion: float = 0):\n",
    "    # query\n",
    "    query = impression[0][\"query\"]\n",
    "    assert all([i[\"query\"] == query for i in impression])\n",
    "    search_id = impression[0][\"search_id\"]\n",
    "\n",
    "    # click\n",
    "    strong_pos = []\n",
    "    pos = []\n",
    "    neg = []\n",
    "    strong_neg = []\n",
    "    strong_pos_position = []\n",
    "\n",
    "    neg_candidates = []\n",
    "    for i, x in enumerate(impression):\n",
    "        text = x[\"doc_info\"]\n",
    "        doc_id = x[\"doc_id\"]\n",
    "\n",
    "        if x[\"search_result_click_cnt\"] > 0:\n",
    "            strong_pos.append((text, doc_id))\n",
    "            strong_pos_position.append(x[\"position\"])\n",
    "        elif x[\"play_time_max\"] > 10000:\n",
    "            pos.append((text, doc_id))\n",
    "        else:\n",
    "            neg_candidates.append(x)\n",
    "\n",
    "    if len(strong_pos) < 1:\n",
    "        return None\n",
    "\n",
    "    min_click_position = min(strong_pos_position)\n",
    "    for i, x in enumerate(neg_candidates):\n",
    "        text = x[\"doc_info\"]\n",
    "        doc_id = x[\"doc_id\"]\n",
    "        position = x[\"position\"]\n",
    "\n",
    "        if x[\"play_time_max\"] < 3000:\n",
    "            if position < min_click_position:\n",
    "                strong_neg.append((text, doc_id))\n",
    "            else:\n",
    "                neg.append((text, doc_id))\n",
    "\n",
    "    if doc_as_query_portion > 0 and random.uniform(0, 1) <= doc_as_query_portion:\n",
    "        query, _ = strong_pos.pop(0)\n",
    "        result_pos = strong_pos + pos\n",
    "        result_neg = strong_neg + neg\n",
    "    else:\n",
    "        result_pos = strong_pos + pos\n",
    "        result_neg = strong_neg + neg\n",
    "\n",
    "    if len(result_pos) < 1 or len(result_neg) < 1:\n",
    "        return None\n",
    "\n",
    "    result_pos = result_pos[:3]\n",
    "    result_neg = result_neg[:3]\n",
    "\n",
    "    return {\n",
    "        \"search_id\": search_id,\n",
    "        \"query\": query,\n",
    "        \"pos\": [p[0] for p in result_pos],\n",
    "        \"neg\": [n[0] for n in result_neg],\n",
    "        \"pos_ids\": [p[1] for p in result_pos],\n",
    "        \"neg_ids\": [n[1] for n in result_neg]\n",
    "    }\n",
    "\n",
    "\n",
    "def process_impression_data(batch, doc_as_query_portion):\n",
    "    results = []\n",
    "    for impression in batch:\n",
    "        result = collate_query_pos_neg_from_impression(impression, doc_as_query_portion)\n",
    "        if result is not None:\n",
    "            results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def generate_impressions(dataset, doc_as_query_portion, num_workers, max_batches_in_memory=10):\n",
    "    all_batches = []\n",
    "    current_batch = []\n",
    "    prev_search_id = None\n",
    "    futures = []\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "        for x in tqdm(dataset):\n",
    "            search_id = x[\"search_id\"]\n",
    "            if search_id != prev_search_id and prev_search_id is not None:\n",
    "                # Impression完成，加入当前batch\n",
    "                all_batches.append(current_batch)\n",
    "                current_batch = []\n",
    "\n",
    "                # 如果达到阈值，提交任务\n",
    "                if len(all_batches) >= max_batches_in_memory:\n",
    "                    futures.append(executor.submit(process_impression_data, all_batches, doc_as_query_portion))\n",
    "                    all_batches = []  # 清空缓存\n",
    "\n",
    "            current_batch.append(x)\n",
    "            prev_search_id = search_id\n",
    "\n",
    "        # 处理剩余数据\n",
    "        if current_batch:\n",
    "            all_batches.append(current_batch)\n",
    "        if all_batches:\n",
    "            futures.append(executor.submit(process_impression_data, all_batches, doc_as_query_portion))\n",
    "\n",
    "        # 收集任务结果\n",
    "        for future in tqdm(as_completed(futures)):\n",
    "            for result in future.result():\n",
    "                yield result\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "num_workers = cpu_count()\n",
    "# num_workers = 1\n",
    "# print(num_workers)\n",
    "new_dataset = Dataset.from_generator(lambda: generate_impressions(dataset, 0, num_workers, max_batches_in_memory=num_workers * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['search_id', 'query', 'pos', 'neg', 'pos_ids', 'neg_ids'],\n",
       "    num_rows: 19441\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "fileId": "46249074-2830-4650-b35e-5c0303685660",
  "filePath": "/opt/tiger/DouyinSearchEmb/notebooks/prepare_contra_neg.ipynb",
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
