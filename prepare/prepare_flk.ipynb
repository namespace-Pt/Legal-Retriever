{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import docx\n",
    "import time\n",
    "import requests\n",
    "import datetime\n",
    "import urllib.parse as urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "def timestamp():\n",
    "    now = datetime.datetime.now()\n",
    "    # 将datetime对象转换为时间戳（以秒为单位）\n",
    "    timestamp_seconds = time.mktime(now.timetuple())\n",
    "    # 再将秒转换为毫秒，乘以1000得到13位的时间戳\n",
    "    timestamp_milliseconds = int(timestamp_seconds * 1000)\n",
    "    return timestamp_milliseconds\n",
    "\n",
    "\n",
    "def get_data(url, headers, data=None):\n",
    "    first_time = True\n",
    "    while first_time or resp is None or resp.status_code != 200:\n",
    "        first_time = False\n",
    "        try:\n",
    "            if data is None:\n",
    "                resp = requests.get(url, headers=headers, timeout=5)\n",
    "            else:\n",
    "                resp = requests.post(url, headers=headers, data=data, timeout=5)\n",
    "        except requests.exceptions.Timeout:\n",
    "            resp = None\n",
    "            continue\n",
    "        time.sleep(1)\n",
    "        print(f\"failed {url} data: {data}\")\n",
    "    return resp.json()\n",
    "\n",
    "\n",
    "base_dir = \"/data/peitian/Data/legal/flk\"\n",
    "raw_dir = os.path.join(base_dir, \"raw\")\n",
    "output_dir = os.path.join(base_dir, \"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = \"https://flk.npc.gov.cn\"\n",
    "download_host = \"https://wb.flk.npc.gov.cn\"\n",
    "api_url = urlparse.urljoin(host, \"./api/detail\")\n",
    "\n",
    "headers = {\n",
    "    \"accept\": \"application/json, text/javascript, */*; q=0.01\",\n",
    "    \"accept-language\": \"zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6\",\n",
    "    \"sec-ch-ua\": \"\\\"Microsoft Edge\\\";v=\\\"131\\\", \\\"Chromium\\\";v=\\\"131\\\", \\\"Not_A Brand\\\";v=\\\"24\\\"\",\n",
    "    \"sec-ch-ua-mobile\": \"?0\",\n",
    "    \"sec-ch-ua-platform\": \"\\\"macOS\\\"\",\n",
    "    \"sec-fetch-dest\": \"empty\",\n",
    "    \"sec-fetch-mode\": \"cors\",\n",
    "    \"sec-fetch-site\": \"same-origin\",\n",
    "    \"x-requested-with\": \"XMLHttpRequest\",\n",
    "    \"cookie\": \"wzws_sessionid=gmZhYjg1ZqBnZNRogDM2LjExMC4xNjMuNjWBY2E5ZDdi; Hm_lvt_54434aa6770b6d9fef104d146430b53b={timestamp}; HMACCOUNT=6B3CB05578CC9581; Hm_lpvt_54434aa6770b6d9fef104d146430b53b={timestamp}\",\n",
    "    \"referer\": \"https://flk.npc.gov.cn/fl.html\",\n",
    "    \"referrer-policy\": \"strict-origin-when-cross-origin\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = timestamp()\n",
    "headers[\"cookie\"] = headers[\"cookie\"].format(timestamp=ts)\n",
    "\n",
    "num_files = len(os.listdir(raw_dir))\n",
    "\n",
    "page_path = os.path.join(raw_dir, \"pages.txt\")\n",
    "\n",
    "with open(page_path, \"r\") as f:\n",
    "    start_idx = int(f.read().strip().split(\"\\n\")[-1])\n",
    "\n",
    "with open(page_path, \"a+\") as g:\n",
    "    for i in range(start_idx, 70):\n",
    "        print(f\"Curling page {i}...\")\n",
    "\n",
    "        page_url = f\"https://flk.npc.gov.cn/api/?type=flfg&searchType=title%3Bvague&sortTr=f_bbrq_s%3Bdesc&gbrqStart=&gbrqEnd=&sxrqStart=&sxrqEnd=&sort=true&page={i}&size=10&_={ts}\"\n",
    "        page_headers = headers\n",
    "        data = get_data(page_url, page_headers)\n",
    "\n",
    "        for item in tqdm(data[\"result\"][\"data\"], desc=\"Enumerating Items\"):\n",
    "            # item = detail_list[0]\n",
    "            title = item[\"title\"]\n",
    "\n",
    "            detail_url = urlparse.urljoin(host, item[\"url\"])\n",
    "\n",
    "            api_headers = headers.copy()\n",
    "            api_headers[\"origin\"] = host\n",
    "            api_headers[\"referer\"] = detail_url\n",
    "            api_headers[\"content-type\"] = \"application/x-www-form-urlencoded; charset=UTF-8\"\n",
    "\n",
    "            data = get_data(api_url, api_headers, data=urlparse.urlencode({\"id\": item[\"id\"]}))\n",
    "            word_path = [x for x in data[\"result\"][\"body\"] if x[\"type\"] == \"WORD\"]\n",
    "            if len(word_path):\n",
    "                path = word_path[0][\"path\"]\n",
    "                ext = \"docx\"\n",
    "            else:\n",
    "                path = [x for x in data[\"result\"][\"body\"] if x[\"type\"] == \"HTML\"][0][\"url\"]\n",
    "                ext = \"html\"\n",
    "            download_url = urlparse.urljoin(download_host, path)\n",
    "\n",
    "            download_resp = requests.get(download_url)\n",
    "            try:\n",
    "                with open(os.path.join(raw_dir, f\"{title}.{ext}\"), \"wb\") as f:\n",
    "                    f.write(download_resp.content)\n",
    "            except OSError:\n",
    "                pass\n",
    "            time.sleep(1)\n",
    "\n",
    "        time.sleep(1)\n",
    "        \n",
    "        g.write(str(i) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "SECTION_PATTERN = \"(\\s*第[一二三四五六七八九十百千]+章\\s+.*?\\n)\"\n",
    "ARTICLE_PATTERN = \"(\\s*第[一二三四五六七八九十百千]+条)\"\n",
    "\n",
    "def read_docx_file(file_path):\n",
    "    try:\n",
    "        doc = docx.Document(file_path)\n",
    "    except:\n",
    "        return \"\"\n",
    "    full_text = []\n",
    "    for paragraph in doc.paragraphs:\n",
    "        full_text.append(paragraph.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "def process_html(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(content, \"lxml\")\n",
    "    result = []\n",
    "    rules = soup.find_all(\"p\", class_=\"law-rule-text\")\n",
    "    current_article = None\n",
    "    \n",
    "    for rule in rules:\n",
    "        num_span = rule.find(\"span\", class_=\"law-rule-num\")\n",
    "        text_span = rule.find(\"span\", class_=\"rule-text\")\n",
    "        if num_span and text_span:\n",
    "            # 新条目\n",
    "            if current_article:\n",
    "                result.append(current_article)\n",
    "            current_article = {\"content\": text_span.text.strip(), \"law_section\": None, \"law_article\": num_span.text.strip()}\n",
    "        elif current_article:\n",
    "            # 合并到当前条目\n",
    "            current_article[\"content\"] += \"\\n\" + rule.text.strip()\n",
    "    \n",
    "    if current_article:\n",
    "        result.append(current_article)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def process_docx(path):\n",
    "    content = read_docx_file(path)\n",
    "    content = content.replace(\"\\u3000\", \" \")\n",
    "\n",
    "    # 判断是否包含章节信息\n",
    "    has_sections = bool(re.search(SECTION_PATTERN, content))\n",
    "    result = []\n",
    "    current_article = None\n",
    "    current_section = None\n",
    "        \n",
    "    if has_sections:\n",
    "        # 按章节和条文划分\n",
    "        sections = re.split(SECTION_PATTERN, content)\n",
    "        for section in sections:\n",
    "            section_match = re.search(SECTION_PATTERN, section)\n",
    "            if section_match:\n",
    "                current_section = section_match.group().strip()\n",
    "            # ['第一条', 'xxx', '第二条', 'xxx', ...]\n",
    "            articles = re.split(ARTICLE_PATTERN, section)\n",
    "            article_contents = []\n",
    "            is_first_match = True\n",
    "            # print(f\"Fuck {current_section} {articles}\")\n",
    "            # NOTE: manually add an auxiliary article so that all articles are processed by the same logic\n",
    "            articles.append(\"第四条\")\n",
    "            for article in articles:\n",
    "                article = article.strip()\n",
    "                if len(article) == 0:\n",
    "                    continue\n",
    "                article_match = re.search(ARTICLE_PATTERN, article)\n",
    "                # NOTE: the first article_match indicates an empty article_contents, ignore it\n",
    "                if article_match:\n",
    "                    if not is_first_match:\n",
    "                        result.append({\"content\": \"\\n\".join(article_contents), \"law_section\": current_section, \"law_article\": current_article})\n",
    "                    article_contents.clear()\n",
    "                    current_article = article_match.group().strip()\n",
    "                    is_first_match = False\n",
    "                # NOTE: split text to unify formats\n",
    "                article_contents.extend([x for x in re.split(\"\\s+\", article.strip()) if len(x)])\n",
    "                # print(f\"Shit {article} article_contents {article_contents}\")\n",
    "    else:\n",
    "        # 仅按条文划分\n",
    "        # ['第一条', 'xxx', '第二条', 'xxx', ...]\n",
    "        articles = re.split(ARTICLE_PATTERN, content)\n",
    "        # NOTE: manually add an auxiliary article so that all articles are processed by the same logic\n",
    "        articles.append(\"第四条\")\n",
    "        article_contents = []\n",
    "        is_first_match = True\n",
    "        # print(f\"Fuck {current_section} {articles}\")\n",
    "        for article in articles:\n",
    "            article = article.strip()\n",
    "            if len(article) == 0:\n",
    "                continue\n",
    "            article_match = re.search(ARTICLE_PATTERN, article)\n",
    "            if article_match:\n",
    "                # NOTE: the first article_match indicates an empty article_contents, ignore it\n",
    "                if not is_first_match:\n",
    "                    result.append({\"content\": \"\\n\".join(article_contents), \"law_section\": current_section, \"law_article\": current_article})\n",
    "                article_contents.clear()\n",
    "                current_article = article_match.group().strip()\n",
    "                is_first_match = False\n",
    "            # NOTE: split text to unify formats\n",
    "            article_contents.extend([x for x in re.split(\"\\s+\", article.strip()) if len(x)])\n",
    "            # print(f\"Shit {article} article_contents {article_contents}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 559/559 [00:06<00:00, 80.95it/s] \n"
     ]
    }
   ],
   "source": [
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for file_name in tqdm(os.listdir(raw_dir)):\n",
    "    src_path = os.path.join(raw_dir, file_name)\n",
    "    law_name, ext = file_name.split(\".\")\n",
    "    dest_path = os.path.join(output_dir, f\"{law_name}.jsonl\")\n",
    "    \n",
    "    if not law_name.endswith(\"法\"):\n",
    "        continue\n",
    "    \n",
    "    with open(dest_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        # if ext == \"html\":\n",
    "        #     results = process_html(src_path)\n",
    "        # else:\n",
    "        results = process_docx(src_path)\n",
    "        \n",
    "        for res in results:\n",
    "            res[\"law_name\"] = law_name\n",
    "            f.write(json.dumps(res, ensure_ascii=False) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
